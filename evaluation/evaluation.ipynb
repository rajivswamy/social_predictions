{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp4hTyiMLvgc"
      },
      "source": [
        "# Evaluation Pipeline Construction\n",
        "\n",
        "## High Level Todos\n",
        "- Establish guidelines\n",
        "    - columns for attributes should be named `attribute_{NAME}`, if more than 2 groups in the attribute can be coded {0,1,2,...}\n",
        "    - make one function that can process one csv file, evaluate_file\n",
        "    - make another function that can do batch processing using the single file\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "i2ianE9WLvge"
      },
      "outputs": [],
      "source": [
        "# General Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pathlib\n",
        "import json\n",
        "\n",
        "from fairlearn.metrics import (\n",
        "    MetricFrame,\n",
        "    selection_rate,\n",
        "    false_positive_rate, # false positive error rate balance\n",
        "    true_positive_rate, # false negative error rate balance\n",
        ")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, # use for both performance and fairness parts\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ITNzfkLvgf"
      },
      "source": [
        "## Synthetic CSV Generation, for testing evaluation pipeline\n",
        "\n",
        "input: file_name, num_rows\n",
        "output: none"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kePywmuqLvgf"
      },
      "outputs": [],
      "source": [
        "def generate_binary_prediction_csv(file_path, data_size = 1000):\n",
        "    # Generate random data for y_true, y_pred, and attribute_gender\n",
        "    y_true = np.random.randint(2, size=data_size)\n",
        "    y_pred = np.random.randint(2, size=data_size)\n",
        "    attribute_gender = np.random.randint(2, size=data_size) # assume binary attribute\n",
        "\n",
        "    # Create a DataFrame using pandas\n",
        "    df = pd.DataFrame({\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "        'attribute_gender': attribute_gender\n",
        "    })\n",
        "\n",
        "    # Write the DataFrame to a CSV file\n",
        "    df.to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U_V6MMYBLvgf"
      },
      "outputs": [],
      "source": [
        "generate_binary_prediction_csv('synthetic_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtieOVcLLvgg"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Check what metrics only good for binary attributes\n",
        "\n",
        "Aggregate Performance:\n",
        "- accuracy\n",
        "- f1-score\n",
        "- precision\n",
        "- recall\n",
        "\n",
        "Fairness Metrics:\n",
        "- Disparity\n",
        "- predictive value parity\n",
        "- Equalized Odds (Error Rate balance)\n",
        "- Accuracy equality\n",
        "- Treatment Equality\n",
        "\n",
        "\n",
        "Further Ideas:\n",
        "- should we process class probability labels\n",
        "\n",
        "Functions:\n",
        "- process file\n",
        "    - input: file_name, assume it follows the formate assumptions, assume model_name is the file name before csv\n",
        "    - output: returns a dict\n",
        "- process folder (batch processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mtiBwkipLvgg"
      },
      "outputs": [],
      "source": [
        "# Quick utility function to get confusion matrix\n",
        "def _get_conf_mat_values(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "  # Extracting confusion matrix values\n",
        "  TN = cm[0, 0]\n",
        "  FP = cm[0, 1]\n",
        "  FN = cm[1, 0]\n",
        "  TP = cm[1, 1]\n",
        "\n",
        "  return TN, FP, FN, TP\n",
        "\n",
        "def predictive_parity(y_true, y_pred):\n",
        "  TN, FP, FN, TP = _get_conf_mat_values(y_true, y_pred)\n",
        "  return TP/(TP+FP)\n",
        "\n",
        "def error_rate_ratio(y_true, y_pred):\n",
        "  TN, FP, FN, TP = _get_conf_mat_values(y_true, y_pred)\n",
        "  return FN/FP\n",
        "\n",
        "def evaluate_file(path_obj):\n",
        "    df = pd.read_csv(path_obj)\n",
        "\n",
        "    model_name = path_obj.name\n",
        "\n",
        "    y_true = df['y_true']\n",
        "    y_pred = df['y_pred']\n",
        "    sensitive_attribute = df['attribute_gender']\n",
        "\n",
        "    # Fairness measurements processing\n",
        "    metrics = {\n",
        "              'selection_rate': selection_rate,\n",
        "              'ppv': predictive_parity,\n",
        "              'fp_err_rate_balance': false_positive_rate,\n",
        "              'tp_error_rate_balance': true_positive_rate,\n",
        "              'accuracy': accuracy_score,\n",
        "              'error_rate_ratio': error_rate_ratio,\n",
        "               }\n",
        "\n",
        "    mf = MetricFrame(\n",
        "                    metrics=metrics,\n",
        "                    y_true=y_true,\n",
        "                    y_pred=y_pred,\n",
        "                    sensitive_features=sensitive_attribute\n",
        "                    )\n",
        "\n",
        "    results = {\n",
        "        'model_performance': {'accuracy': accuracy_score(y_true, y_pred),\n",
        "                              'f1_score': f1_score(y_true, y_pred),\n",
        "                              'precision': precision_score(y_true, y_pred),\n",
        "                              'recall': recall_score(y_true, y_pred),\n",
        "                              },\n",
        "        'fairness_performance': {\n",
        "            'by_group_data': mf.by_group.to_dict(), # raw data\n",
        "            'difference': mf.difference().to_dict(), # max inter-group diff per stat\n",
        "            },\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def batch_evaluate(folder_path, write_name = None):\n",
        "  \"\"\"\n",
        "  Processes model result CSV files in the\n",
        "\n",
        "  input: folder_path: str\n",
        "  returns: list of dicts containing performance results\n",
        "  \"\"\"\n",
        "  folder = pathlib.Path(folder_path)\n",
        "\n",
        "  perf_data = {}\n",
        "\n",
        "  # Iterate through all the csv files in the folder\n",
        "  for path in list(folder.glob('*.csv')):\n",
        "    perf_data[str(path)] = evaluate_file(path)\n",
        "\n",
        "  # Write data to 'write_name' json file\n",
        "  if write_name is not None:\n",
        "    with open(str(folder/write_name), \"w\") as outfile:\n",
        "      json.dump(perf_data, outfile, indent=4)\n",
        "\n",
        "  return perf_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "EVA6sk4Nrwkw"
      },
      "outputs": [],
      "source": [
        "data = batch_evaluate('./model_data', 'perf_data.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaNMgQZItSCE",
        "outputId": "0b0b2b79-0e15-4092-f777-1071b3ba9113"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/content/model_path')"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pathlib.Path('./model_path').resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjC5MPPwtpjg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cos598 [~/.conda/envs/cos598/]",
      "language": "python",
      "name": "conda_cos598"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
