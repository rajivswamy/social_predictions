{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp4hTyiMLvgc"
      },
      "source": [
        "# Evaluation Pipeline Construction\n",
        "\n",
        "## High Level Todos\n",
        "- Establish guidelines\n",
        "    - columns for attributes should be named `attribute_{NAME}`, if more than 2 groups in the attribute can be coded {0,1,2,...}\n",
        "    - make one function that can process one csv file, evaluate_file\n",
        "    - make another function that can do batch processing using the single file\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i2ianE9WLvge"
      },
      "outputs": [],
      "source": [
        "# General Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pathlib\n",
        "import json\n",
        "\n",
        "from fairlearn.metrics import (\n",
        "    MetricFrame,\n",
        "    selection_rate,\n",
        "    false_positive_rate, # false positive error rate balance\n",
        "    true_positive_rate, # false negative error rate balance\n",
        ")\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, # use for both performance and fairness parts\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ITNzfkLvgf"
      },
      "source": [
        "## Synthetic CSV Generation, for testing evaluation pipeline\n",
        "\n",
        "input: file_name, num_rows\n",
        "output: none"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kePywmuqLvgf"
      },
      "outputs": [],
      "source": [
        "def generate_binary_prediction_csv(file_path, data_size = 1000):\n",
        "    # Generate random data for y_true, y_pred, and attribute_gender\n",
        "    y_true = np.random.randint(2, size=data_size)\n",
        "    y_pred = np.random.randint(2, size=data_size)\n",
        "    attribute_gender = np.random.randint(2, size=data_size) # assume binary attribute\n",
        "\n",
        "    # Create a DataFrame using pandas\n",
        "    df = pd.DataFrame({\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "        'attribute_gender': attribute_gender\n",
        "    })\n",
        "\n",
        "    # Write the DataFrame to a CSV file\n",
        "    df.to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U_V6MMYBLvgf"
      },
      "outputs": [],
      "source": [
        "generate_binary_prediction_csv('synthetic_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtieOVcLLvgg"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Check what metrics only good for binary attributes\n",
        "\n",
        "Aggregate Performance:\n",
        "- accuracy\n",
        "- f1-score\n",
        "- precision\n",
        "- recall\n",
        "\n",
        "Fairness Metrics:\n",
        "- Disparity\n",
        "- predictive value parity\n",
        "- Equalized Odds (Error Rate balance)\n",
        "- Accuracy equality\n",
        "- Treatment Equality\n",
        "\n",
        "\n",
        "Further Ideas:\n",
        "- should we process class probability labels\n",
        "\n",
        "Functions:\n",
        "- process file\n",
        "    - input: file_name, assume it follows the formate assumptions, assume model_name is the file name before csv\n",
        "    - output: returns a dict\n",
        "- process folder (batch processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mtiBwkipLvgg"
      },
      "outputs": [],
      "source": [
        "# Quick utility function to get confusion matrix\n",
        "def _get_conf_mat_values(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "  # Extracting confusion matrix values\n",
        "  TN = cm[0, 0]\n",
        "  FP = cm[0, 1]\n",
        "  FN = cm[1, 0]\n",
        "  TP = cm[1, 1]\n",
        "\n",
        "  return TN, FP, FN, TP\n",
        "\n",
        "def predictive_parity(y_true, y_pred):\n",
        "  TN, FP, FN, TP = _get_conf_mat_values(y_true, y_pred)\n",
        "  return TP/(TP+FP)\n",
        "\n",
        "def error_rate_ratio(y_true, y_pred):\n",
        "  TN, FP, FN, TP = _get_conf_mat_values(y_true, y_pred)\n",
        "  return FN/FP\n",
        "\n",
        "def evaluate_file(path_obj):\n",
        "    df = pd.read_csv(path_obj)\n",
        "\n",
        "    model_name = path_obj.name\n",
        "\n",
        "    y_true = df['y_true']\n",
        "    y_pred = df['y_pred']\n",
        "    sensitive_attribute = df['attribute_gender']\n",
        "\n",
        "    # Fairness measurements processing\n",
        "    metrics = {\n",
        "              'selection_rate': selection_rate,\n",
        "              'ppv': predictive_parity,\n",
        "              'fp_err_rate_balance': false_positive_rate,\n",
        "              'tp_error_rate_balance': true_positive_rate,\n",
        "              'accuracy': accuracy_score,\n",
        "              'error_rate_ratio': error_rate_ratio,\n",
        "               }\n",
        "\n",
        "    mf = MetricFrame(\n",
        "                    metrics=metrics,\n",
        "                    y_true=y_true,\n",
        "                    y_pred=y_pred,\n",
        "                    sensitive_features=sensitive_attribute\n",
        "                    )\n",
        "\n",
        "    results = {\n",
        "        'model_performance': {'accuracy': accuracy_score(y_true, y_pred),\n",
        "                              'f1_score': f1_score(y_true, y_pred),\n",
        "                              'precision': precision_score(y_true, y_pred),\n",
        "                              'recall': recall_score(y_true, y_pred),\n",
        "                              },\n",
        "        'fairness_performance': {\n",
        "            'by_group_data': mf.by_group.to_dict(), # raw data\n",
        "            'difference': mf.difference().to_dict(), # max inter-group diff per stat\n",
        "            },\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def batch_evaluate(folder_path, write_name = None):\n",
        "  \"\"\"\n",
        "  Processes model result CSV files in the\n",
        "\n",
        "  input: folder_path: str\n",
        "  returns: list of dicts containing performance results\n",
        "  \"\"\"\n",
        "  folder = pathlib.Path(folder_path)\n",
        "\n",
        "  perf_data = {}\n",
        "\n",
        "  # Iterate through all the csv files in the folder\n",
        "  for path in list(folder.glob('*.csv')):\n",
        "    perf_data[str(path)] = evaluate_file(path)\n",
        "\n",
        "  # Write data to 'write_name' json file\n",
        "  if write_name is not None:\n",
        "    with open(str(folder/write_name), \"w\") as outfile:\n",
        "      json.dump(perf_data, outfile, indent=4)\n",
        "\n",
        "  return perf_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "EVA6sk4Nrwkw"
      },
      "outputs": [],
      "source": [
        "data = batch_evaluate('./model_data', 'perf_data.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaNMgQZItSCE",
        "outputId": "0b0b2b79-0e15-4092-f777-1071b3ba9113"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/content/model_path')"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pathlib.Path('./model_path').resolve()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constructing Multiplicity Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JjC5MPPwtpjg"
      },
      "outputs": [],
      "source": [
        "import eval as ev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_path = './synth_data/'\n",
        "\n",
        "ev.batch_generate_binary_prediction_csv(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_folder(folder_path):\n",
        "    folder = pathlib.Path(folder_path)\n",
        "    data_files = list(folder.glob('*.csv'))\n",
        "\n",
        "    dframes = [pd.read_csv(path_obj) for path_obj in data_files]\n",
        "\n",
        "    return dframes\n",
        "\n",
        "\n",
        "def compute_ambiguity(dframes, group = None, attribute_name = None):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    dframes: list of dataframes\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # If group is specified, then limit view to protected group\n",
        "    if group is not None:\n",
        "        for df in dframes:\n",
        "            data.append(df[df[attribute_name]==group])\n",
        "    else:\n",
        "        data = dframes\n",
        "    \n",
        "    all_preds = np.array([df['y_pred'].to_numpy() for df in data])\n",
        "\n",
        "    # num_models, num_preds = all_preds.shape --> should hold\n",
        "    # compute number of unique values for each column\n",
        "    unique_counts = np.array([len(np.unique(all_preds[:, i])) for i in range(all_preds.shape[1])])\n",
        "\n",
        "    return (unique_counts > 1).mean()\n",
        "\n",
        "\n",
        "def compute_discrepancy(dframes, group = None, attribute_name = None):\n",
        "    data = []\n",
        "\n",
        "    # If group is specified, then limit view to protected group\n",
        "    if group is not None:\n",
        "        for df in dframes:\n",
        "            data.append(df[df[attribute_name]==group])\n",
        "    else:\n",
        "        data = dframes\n",
        "    \n",
        "    all_preds = np.array([df['y_pred'].to_numpy() for df in data])\n",
        "    num_models, num_preds = all_preds.shape\n",
        "\n",
        "    max_disc = 0\n",
        "\n",
        "    # Pass through all model pairings to compute discrepancy\n",
        "    for i in range(num_models):\n",
        "        for j in range(i,num_models):\n",
        "            disagree = (all_preds[i] != all_preds[j]).sum()\n",
        "\n",
        "            # Change return if needed\n",
        "            max_disc = max(max_disc, disagree)\n",
        "    \n",
        "    return max_disc/num_preds\n",
        "\n",
        "def evaluate_model_multiplicity(folder_path, attr_list):\n",
        "    dframes = load_data_folder(folder_path)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # compute total amgig, disc metrics\n",
        "    results[\"aggregate\"] = {\n",
        "        \"ambiguity\": compute_ambiguity(dframes),\n",
        "        \"discrepancy\": compute_discrepancy(dframes)\n",
        "    }\n",
        "\n",
        "    # compute group level metrics for each attr\n",
        "    results[\"attribute\"] = {}\n",
        "\n",
        "    for attr in attr_list:\n",
        "        results[\"attribute\"][attr] = {}\n",
        "        for group in [0,1]:\n",
        "            results[\"attribute\"][attr][group] = {\n",
        "                \"ambiguity\": compute_ambiguity(dframes, group, attr),\n",
        "                \"discrepancy\": compute_discrepancy(dframes, group, attr)\n",
        "            }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs = load_data_folder(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 attribute_gender\n",
            "1 attribute_gender\n"
          ]
        }
      ],
      "source": [
        "results = evaluate_model_multiplicity(base_path, ['attribute_gender'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'aggregate': {'ambiguity': 1.0, 'discrepancy': 0.561},\n",
              " 'attribute': {'attribute_gender': {0: {'ambiguity': 1.0,\n",
              "    'discrepancy': 0.591182364729459},\n",
              "   1: {'ambiguity': 1.0, 'discrepancy': 0.5788423153692615}}}}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Gen Perf, Fairness, and Mult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import eval as ev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_path = './synth_data/'\n",
        "\n",
        "results = ev.batch_evaluate(base_path, ['attribute_gender'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reworking Evaluation Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rajivswamy/miniforge3/envs/cos598/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "predictions_path = \"/Users/rajivswamy/Desktop/COS598I/Final_Proj/social_predictions/predictions_dec_paths_train+test/predictions/binary_feature_sample_19_features_train_predictions_train.csv\"\n",
        "targets_path = \"/Users/rajivswamy/Desktop/COS598I/Final_Proj/social_predictions/binary_feature_samples/binary_feature_sample_19_targets_train.csv\"\n",
        "sensitive_attribute_path = \"\" # Placeholder don't have right now\n",
        "\n",
        "res = ev.batch_evaluate_v2(predictions_path, targets_path, sensitive_attribute_path, write_path=\"test_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Experiment Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder = '../experiments_final/'\n",
        "sample_list = [6,10,11,15,28,38,40,45,47]\n",
        "\n",
        "def process_experiment(experiment_path, sample_list, write_out=False):\n",
        "\n",
        "    sample_results = {}\n",
        "    for sample in sample_list:\n",
        "        print(sample)\n",
        "        # Common data\n",
        "        test_targets_path = os.path.join(experiment_path,f\"binary_feature_samples/binary_feature_sample_{sample}_targets_test.csv\")\n",
        "        train_targets_path  = os.path.join(experiment_path,f\"binary_feature_samples/binary_feature_sample_{sample}_targets_train.csv\")\n",
        "        test_features_path = os.path.join(experiment_path,f\"binary_feature_samples/binary_feature_sample_{sample}_features_test.csv\")\n",
        "        train_features_path = os.path.join(experiment_path,f\"binary_feature_samples/binary_feature_sample_{sample}_features_train.csv\")\n",
        "        \n",
        "        # AP Predictions\n",
        "        aptest_preds_path = os.path.join(experiment_path, f\"predictions/binary_feature_sample_{sample}_features_train_predictions_test.csv\")\n",
        "        aptrain_preds_path = os.path.join(experiment_path, f\"predictions/binary_feature_sample_{sample}_features_train_predictions_train.csv\")\n",
        "        \n",
        "        # CP Predictions, only test data\n",
        "        cptest_preds_path = os.path.join(experiment_path,f\"cp_draws/binary_feature_sample_{sample}_original_f_test\")\n",
        "\n",
        "\n",
        "        # Run eval script on AP\n",
        "        train_AP_eval = ev.batch_evaluate_v2(aptrain_preds_path, train_targets_path, train_features_path)\n",
        "        test_AP_eval = ev.batch_evaluate_v2(aptest_preds_path, test_targets_path, test_features_path)\n",
        "\n",
        "        # Run eval script on CP\n",
        "        test_CP_eval = ev.batch_evaluate_v2(cptest_preds_path, test_targets_path, test_features_path, cp=True)\n",
        "        \n",
        "        sample_results[sample] = {\n",
        "            \"train_AP_eval\": train_AP_eval,\n",
        "            \"test_AP_eval\": test_AP_eval,\n",
        "            \"test_CP_eval\": test_CP_eval,\n",
        "        }\n",
        "    \n",
        "    if write_out:\n",
        "        with open(os.path.join(experiment_path,\"eval.json\"), \"w\") as outfile:\n",
        "            json.dump(sample_results, outfile, indent=4)\n",
        "    \n",
        "    return sample_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "10\n",
            "11\n",
            "15\n",
            "28\n",
            "38\n",
            "40\n"
          ]
        }
      ],
      "source": [
        "res = process_experiment(folder, sample_list, write_out=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
